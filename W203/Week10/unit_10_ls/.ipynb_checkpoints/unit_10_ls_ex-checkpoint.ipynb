{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Unit 10 Live Session </center> </h1>\n",
    "<h3> W203 Instructional Team </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simple Linear Regression </h2>\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/linear_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Class Announcements\n",
    "1. Announcement 1\n",
    "2. Announcement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 0.2 Getting to This Document</h3>\n",
    "\n",
    "If you have not cloned the unit_9_ls repo yet then on the command line\n",
    "\n",
    "1. git clone https://github.com/w203-spring-19/unit_10_ls.git \n",
    "\n",
    "2. cd unit_10_ls\n",
    "\n",
    "\n",
    "\n",
    "If you have cloned this repo already then on the command line\n",
    "\n",
    "1. cd unit_10_ls\n",
    "\n",
    "2. git fetch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1 Reproducibility Review </h3>\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/significant.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong here? \n",
    "\n",
    "<ul> \n",
    "    Generalization\n",
    "    <ul>\n",
    "        <li> So far we have been concerned with our ability to generalize the conclusions drawn from a single hypothesis test (in isolation) to the population distribution.\n",
    "        <br><br>\n",
    "        <li> Real studies are not carried out in this way, one will use a single data set to conduct an number of different tests/comparisons. Especially if exploration and testing are conducted on the same data set. \n",
    "        <br><br>\n",
    "        <li> Unfortunately, as we saw in this week's async we learned that the number of comparisons conducted, the sequence in which tests are conducted, the manner in which one determines which test they will conduct, and incentives for publishing the results can all have a significant effect on extent to which their results generalize.\n",
    "        <br><br>\n",
    "        <li> Put roughly, the more comparisons you make, the more likely that you will find significant results related to an idiosyncratic feature of the sample rather than a feature of the true population.  \n",
    "    <ul> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2 Reproducibility </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** You have a dataset of the number of Facebook status updates by day of the week.  You run 7 different t-tests, one for posts on Monday (versus all other days), or for Tuesday (versus all other days), etc.  Only the test for Sunday is significant, with a p-value of .045, so you throw out the other tests.  Should you conclude that Sunday has a significant effect on number of posts?  (How can you address this situation responsibly when you publish your results?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** As before, you have a dataset of the number of Facebook status updates by day of the week.  You do a little EDA and notice that Sunday seems to have more \"status updates\" than all other days, so you recode your \"day of the week\" variable into a binary one: Sunday = 1, All other days = 0.  You run a t-test and get a p-value of .045.  Should you conclude that Sunday has a significant effect on number of posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Suppose researcher A tests if Monday has an effect (versus all other days), Researcher B tests Tuesday (versus all other days), and so forth.  Only Researcher G, who tests Sunday finds a significant effect with a p-value of .045.  Only Researcher G gets to publish her work.  If you read the paper, should you conclude that Sunday has a significant effect on number of posts?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** What if researcher G above is a sociologist that chooses to measure the effect of Sunday based on years of observing the way people behave on weekends?  Researcher G is not interested in the other tests, because Sunday is the interesting day from her perspective, and she wouldn't expect any of the other tests to be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Many observers have noted that as studies yielding statistically significant results are repeated, estimated effect sizes go down and often become insignificant.  Why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3  Simple Linear Regression </h3>\n",
    "\n",
    "Suppose we have data, represented by ($X_1$, $Y_1$), . . . , ($X_n$, $Y_n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**  Let $u_i$ be an error term, write a simple regression model for the $i^{th}$ observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** In words, what do the statistical errors $u_i$ represent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** What assumption(s) are needed in order to interpret $\\beta_0 + \\beta_1X_i$ as a conditional expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** Do we want the residuals $\\hat{u}_i = Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1X_i$ to be small in magnitude? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** To define a regression line, is it sufficient to require $\\sum \\hat{u_i} = 0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4 Properties of residuals </h3>\n",
    "\n",
    "We derive our estimator for $\\beta_0$ and $\\beta_1$  by setting our sample moments equal to their theoretical values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** What are the implications of the following properties?\n",
    "1. $n^{-1}\\sum_{i=1}^n \\hat{u_i} = 0$.\n",
    "\n",
    "2. $n^{-1}\\sum_{i=1}^n X_i \\hat{u_i} = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** How many different lines through the X-Y plane would fulfill these two conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** If someone claimed that in their regression the condition \n",
    "$$n^{-1}\\sum_{i=1}^n X_i \\hat{u_i} = 0$$ \n",
    "\n",
    "implied something about the nature of the relationship between $X_i$ and $u_i$ (notice I have removed the hat) what would you say?               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5 Regression in R </h3>\n",
    "When a linear pattern is evident from a scatter plot, the relationship between the two variables is often modeled with a straight line. This line is expressed in a linear model between the response (or dependent) variable and the predictor (or independent) variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are useful for running a linear regression in R.\n",
    "\n",
    "- Fitting a model: model <- lm(y ~ x)\n",
    "- Coefficients: model\\$coef or coef(model)\n",
    "- Fitted values: model$fitted or fitted(model)\n",
    "- Residuals: model\\$resid or resid(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('Gpa.Rdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1**  Before we can find the least square regression line, we need to determine the explanatory and response varibles. Define 2 new variables in R, x and Y, and assign the explanatory and response variables from the dataset, respectively, and conduct a cursory analysis of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Create a scatterplot of CollGPA versus HSGPA and find the correlation between the two variables. What can we infer from the correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** Now that we know a few things about the data, we want to find a line that best represents the relationship between the variables. In other words, we want to draw a slope that comes closest to describing the data.\n",
    "\n",
    "Characterize the equation mathematically (or algebraically **do not use lm()**). Find the least squares estimates of $\\beta_0$ and $\\beta_1$. Corresponding to the model. \n",
    "\n",
    "$$ CollGPA = \\beta_0 + \\beta_1\\cdot HSGPA$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: ** To perform the least square regression is R we can use the lm command. If you are interested use the help(lm) command to learn the different options for using this function. To relationship between the variables is defined in the lm command using a tilde (\"~\") between the vector containing the response variable and the vector containing the explanatory variable: lm(Y ~ x).\n",
    "\n",
    "If you would like to know what else is stored in the variable you can use the attributes command: attributes(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4** Find the least squares estimates of $\\beta_0$ and $\\beta_1$ using the R\n",
    "function lm()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** abline() adds one or more straight lines to the current plot. The arguments to abline() are a=b0 and b=b1. Add the least squares line to the scatterplot created in 1 using the R function abline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6** Compute the sample correlation between $X$ and $\\hat{u}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6 OLS Goodness of Fit </h3>\n",
    "\n",
    "When building regression models, \"goodness-of-fit\" explains how closely our model of the data (i.e. the predictor variables) fits the outcome data. In other words, how much of the variation in an outcome can we explain with a particular model? \n",
    "\n",
    "** R-Squared ** is a measure commonly used for assessing model fit.  It can be understood as the proportion of variance in the outcome that can be accounted for by the model.\n",
    "\n",
    "Looking at our simple bivariate model, we can extract R-squared as a measure of model fit in a number of ways. The easiest is simply to extract it from the lm object using summary(model)\\$r.squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in summary(model): object 'model' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in summary(model): object 'model' not found\nTraceback:\n",
      "1. summary(model)"
     ]
    }
   ],
   "source": [
    "summary(model)$r.squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: We normally discourage students from using the summary command with lm objects.  The reason, as we will see later, is that summary makes a strong assumption called homoskedasticity, which is usually not justified.  However, it is ok to use the command in order to extract R-squared.\n",
    "\n",
    "But we can also calculate R-squared from our data in a number of ways. Take a couple of minutes to manually calculate R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1** By squaring the correlation between X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2** By taking the ratio of the variance of the fitted values to the variance of Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3** By weighting the slope coefficient: $R^2 = \\beta_1^2 \\frac{var(X)}{var(Y)}$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 7 Adjusted R Square </h3>\n",
    "\n",
    "The \"Adjusted R-squared\" is commonly used in place of the \"regular\" R-squared, which is sensitive to the number of independent variables in the model. In other words, as we put more variables into the model, R-squared increases even if those variables are unrelated to the outcome. \n",
    "\n",
    "Adjusted R-squared attempts to correct for this by deflating R-squared by the expected amount of increase from including irrelevant additional predictors. \n",
    "\n",
    "We can see this property of R-squared and Adjusted R-squared by adding a completely random variables unrelated to our other covariates or the outcome into our model and examine the impact on R-squared and Adjusted R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1** Add this variable to your simple regression model, creating a new lm object, then observe what happens to R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2** Now extract the adjusted R-squared from both models using lm\\$adj.r.squared.  It may also go down, but by less than regular r-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 8 OLS: Issues to be Aware of </h3>\n",
    "\n",
    "Unfortunately, the pitfalls of applying least squares are not often well understood by many of the people who attempt to apply it. What follows is a list of some of the biggest problems with using least squares regression in practice, along with some brief comments about how these problems may be mitigated or avoided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers:**  Least squares regression can perform very badly when some points in the training data have excessively large or small values for the dependent variable compared to the rest of the training data. The reason for this is that since the least squares method is concerned with minimizing the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data can have a disproportionately large effect on the resulting constants that are being solved for.\n",
    "\n",
    "**WARNING: Do not ever remove an observation just because it's an outlier.**\n",
    "\n",
    "**8.1** Returning to our example, let's add an outlier $(X,Y) = (1.5,3.4)$ redo the scatter plot and compute the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** What effect does in inclusion of the scatter plot have on the coefficient $\\beta_1$.  Let's see what it does to the linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3** Let's see that scatterplot again with our new regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: Do not ever remove an observation just because it's an outlier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Non-Linearities ** All linear regression methods (including, of course, least squares regression), suffer from the major drawback that in reality most systems are not linear.\n",
    "\n",
    "Let's take another dataset that is clearly non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nl<-seq(-25,25,1)\n",
    "y_nl<-x_nl^2+rnorm(51,0,100)\n",
    "model_nl<-lm(y_nl ~ x_nl)\n",
    "plot(x_nl,y_nl)\n",
    "abline(model_nl, col = \"blue\" , lwd = 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's definitely a relationship here, but we will need to do a transformation prior to OLS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
