---
title: "W203 Unit Summary"
author: "Kevin Hartman"
date: "5/12/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```


## Terms and Definitions
A **population** is a well-defined collection of objects in an area of interest (investigation).

A **census** occurs when desired information is available for all objects in the population.

A **sample** is a subset of the population, selected in some prescribed manner.

A **characteristic** is an attribute of the data, which may be categorical or numerical.

A **variable** is any characteristic whose value may change from one obhject to another in the population.

A **univariate** data set consists of observations on a single variable.

A **multivariate** data set consists of observations on multiple variables.

A **bivariate** data set (a special case of multivariate) is when observations are made on two variables specifically.

**Descriptive Statistics** -- the branch of statistics where an investigator simply wishes to summarize and describe important features of the data.

**Inferential Statistics** -- the branch of statistics where an investigator would like to use the sample information to draw a form of conclusion about the population.


## Unit 1: Descriptive Statistics

**Sample Mean**
<center>$$\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$$</center>
**Sample Median**

After ordering $n$ observations from smallest to largest,
<center>$\tilde{x} = \left\{\begin{array}{lr} \left(\frac{n+1}{2} \right)^{th} \text{ordered value, if n is odd} \\ \text{average of } \left(\frac{n}{2}\right)^{th} \text{and} \left(\frac{n}{2} + 1\right)^{th} \text{ordered values, if n is even} \end{array} \right.$</center>

**Trimmed Mean** - a compromise between $\bar{x}$ and $\tilde{x}$ by eliminating a percentage of the smallest and largest portions of the sample and averaging what remains.

**Range** - the difference betwen the largest and smallest sample variables.

**Deviations from the Mean**

For each value $x_i$ in the sample, its deviation from the mean is $x_i-\bar{x}$

**Sample Variance**

Variance, denoted as $s^2$, is the average squared differences from the mean in a sample set.

<center>$$s^2=\frac{S_{xx}}{n-1}=\frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}$$</center>

It is customary to refer to $s^2$ as being based on ${n-1}$ **degrees of freedom**.

**Sample Standard Deviation**

Standard deviation, denoted as $s$ is the (positive) square root of the variance:

<center>$s = \sqrt{s^2}$</center>


Properties:

Let $x_1, x_2, \dots, x_n$ be a sample and $c$ be any nonzero constant.

1. If $y_1 = x_1 + c, y_2 = x_2 + c,\dots, y_n = x_n + c$, then $s_y^2 = s_x^2$ and
2. If $y_1 = cx_1,\dots,y_n=cx_n$, then $s_y^2=c^2s_x^2$, $s_y=|c|s_x$

where $s_x^2$ is the sample variance of the $x$'s and $s_y^2$ is the sample variance of the $y$'s.


**Upper Fourth** - median of the largest half of observations

**Lower Fourth** - median of the smallest half of observations

**Fourth Spread**

<center>$f_s$ = upper fourth - lower fourth</center>

**Outlier** - any observation farther than $1.5f_s$ from the closest fourth (considered mild if less than $3f_s$)

**Extreme Outlier** - an observation farther than $3f_s$ from the nearest fourth


**Sample Covariance**

The simplest measure of association between two variables.

<center>$cov(x,y)==\frac{\sum_{i=1}^n(x_i-\bar{x})(y_1-\bar{y})}{n-1}$</center>


**Sample Correlation**

Measures the linear relationship betwen to variables

<center>$r_{x,y}=\frac{cov(x,y)}{s_xs_y}$</center>


Properties:

1. Adding $c$ to variable $x$ or variable $y$ doesn't change $r_{x,y}$.
2. Multipling a non-zero $c$ to variable $x$ or variable $y$ doesn't change $r_{x,y}$


## Unit 3: Probability Theory

\usepackage[scr=boondoxo,scrscaled=1.05]{mathalfa}

**Sample space** of an expirement, denoted by $\mathscr{S}$ ,is the set of all possible outcomes of that expirement.

**Event space**, denoted by $\mathscr{F}$, is the subset of outcomes from the sample space. It includes  subsets of $\mathscr{S}$, the empty set $\emptyset$ and $\mathscr{S}$ itself.

**Probability Rule** is a function, $P$, from the set of events to the real numbers

**Axioms**

1. $P(A) >= \emptyset$ for any event $A$ in $\mathscr{F}$
2. $P(\mathscr{S}) = 1$
3. For any countably infite set of disjoint events $\{A_1, A_2, \dots\}$

<center>$P(A_1 \cup A_2 \cup \dots) = \sum_{i=1}^\infty P(A_i)$</center>

**Probability space** is the triple of $(\mathscr{S}, \mathscr{F}, P)$

Properties

1. For any event $A$, $P(A) + P(!A) = 1$, from which $P(A) = 1 - P(!A)$
2. For any event $A$, $P(A) \leq 1$
3. Addition Rule: For any two events $A$ and $B$, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
4. For any three events $A$, $B$, and $C$, $P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$

**Conditional Probability**

1. For any two events $A$ and $B$ with $P(B) > 0$, the conditional probability of $A$ given that $B$ has occured is defined by $P(A|B) = \frac{P(A \cap B)}{P(B)}$
2. Multiplication Rule: $P(A \cap B) = P(B) \cdot P(A|B)$
3. Extending to 3 or more events: $P(A \cap B \cap C) = P(B) \cdot P(A|B) \cdot P(C|A \cap B)$

**Independence**

1. If events $A$ and $B$ are independent from one another, then probability of $A$ given $B$ is the same as the probability of $A$, e.g. $P(A|B) = P(A)$. 
2. Multiplication Rule: $P(A \cap B) = P(A) \cdot P(B)$

**Bayes' Rule**

<center>$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$</center>

**Law of Total Probability**
<center>$P(B) = P(B|A) \cdot P(A) + P(B|!A) \cdot P(!A)$</center>

*Quiz:*

X is a rare disease that affects 2 out of 10,000 people.

Let Y be the event of a positive test.

The best test gives the wrong result 1% of the time for healthy people but is always right for affected people.

If a person tests positive, what is the change that he or she has the disease, P(X|Y)?

$P(X) = 0.0002, P(!X) = 0.9998$

$P(Y|X) = 1$

$P(Y|!X) = 0.01$

$P(Y) = P(Y \cap X) + P(Y \cap !X)$

$P(Y) = P(Y|X) \cdot P(X) + P(Y|!X) \cdot P(!X)$

$P(Y) = (1 \cdot 0.0002) + (0.01 \cdot 0.9998)$

$P(X|Y) = P(Y|X) \cdot P(X) / P(Y)$

$P(X|Y) = \frac{1 \cdot 0.0002}{1 \cdot 0.0002 + 0.01 \cdot 0.9998}$

```{r cakc_this, echo = FALSE}
xy <-{1 * 0.0002} / {1 * 0.0002 + 0.01*0.9998}
xy
```

## Unit 4.1: Discrete Random Variables

Definitions.

Given a Probability space $(\mathscr{S}, \mathscr{F}, P)$, a **random variable** (rv) is a function whose domain is the sample space $\mathscr{S}$ and whose range is the set of real numbers.
<center>$X:\mathscr{S} \rightarrow \mathbb{R}$</center>
e.g. $X(\omega) = x$ 

Any random variable whose only possible values are 0 and 1 is called a **Bernoulli random variable**.

A **discrete set** is a set of disconnected points with no continuous intervals. $O = \left \{ {0, 1, 2, \dots } \right \}$. Binary outcomes with values $O = \left \{ {0, 1} \right \}$ where 1 represents success and has probability $p$ is called a **Bernoulli distribution**.

The **Probability Mass Function** (pmf) $f$ of a **Discrete Random Variable** (discrete rv) $X$ in the problem space $(\mathscr{S}, \mathscr{F}, P)$ is described by 
<center>$f(x) = P(X=x) = P(\left\{w \in \mathscr{S}:X(\omega)=x \right\})$</center>
where the outcomes $O_x = \left \{x \in \mathbb{R} : f(x) > 0 \right \}$

Suppose $p(x)$ depends on a quantity that can be assigned any one of a number of possible values, with each different value determining a different probability distribution. Such a quantity is called a **parameter** of the distribution. The collection of all probability distributions for different values of the parameter is called a **family** of probability distributions.

The **Cumulative Distribution Function** (cdf) $F(X)$ of a **Discrete Random Variable** (discrete rv) $X$ with probability mass function $p(x)$ is described by
<center>$$F(X)=P(X \leq x)=\sum_{y \in O_x : y \leq x}{p(y)} \text{   (the sum of probability masses)}$$</center>
For any number $x$, $F(x)$ is the probability that the observed value of $X$ will be at most $x$.

Example of a probability function for a **bernoulli random variable**, $O_x = \left \{ {0, 1} \right \}$
<center>$$\text{Let } \alpha = P(X=1), f(x;\alpha)= \left\{\begin{array}{lcl} 1-\alpha &; & \mbox{x=0}; \\ \alpha; &; & \mbox{x=1}; \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$</center>


**Expectation of Discrete Random Variables**

Let X be a discrete random variable with outcomes $O = \left \{x_1, x_2, \dots, x_k \right \}$ and probability function $f$

<center>$$\text{Expectation } E(X) = \sum_{j=1}^kx_j \cdot f(x_j)$$</center>

**Expectation of random variables with uniform probability distribution**

Let $X$ be a special case where all outcomes have the same probability, e.g. if $f(x_i)=f(x_j) \forall i,j$

<center>$$\text{Expectation } E(X) = \frac{1}{k}{\sum_j x_j}$$</center>

**Expectation of random variables with binary outcomes**

Let $X$ be Bernoulli random variable that has outcomes 0 and 1, e.g. if $$f(x;\alpha) = \left\{\begin{array}{lcl} 1-\alpha &; & \mbox{x=0}; \\ \alpha; &; & \mbox{x=1}; \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$

<center>Expectation $E(X) = \sum_{x \in {0, 1}}x\cdot f(x) = 0 \cdot (1 - \alpha) + 1 \cdot \alpha = \alpha$</center>

**Expectation of geometric random variables**

Let $X$ be geometric random variable for the number of babies born until the first girl, where $G$ is the event the baby is a girl  $$P(G_i) = g \forall i$$

<center>$$f(1)=P(G_1)=g$$</center>
<center>$$f(2)=P(!G_1 \cap G_2)=(1-g) \cdot g$$</center>
<center>$$f(3)=(1-g)^2 \cdot g$$</center>
<center>$$f(x;g)== \left\{\begin{array}{lcl} (1-g)^{x-1} \cdot g &; & x \in \left \{1, 2, \dots \right \}; \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$</center>

<center>$$\text{Expectation } E(X) = \sum_{x \in \left \{ {1, 2 \dots } \right \}}x \cdot (1-g)^{x-1} \cdot g = \frac{1}{g}$$</center>

**Expectation of functions of random variables**

Let $X$ be a discrete random variable with probability mass function $f$ and $g$ be a function from the real numbers to the real numbers, $g:\mathbb{R} \rightarrow \mathbb{R}$

$g(X)$ is a random variable with probability mass function $f^{'}(x)=P(g(X)=x)=\sum_{y \in O_x, g(y)=x}f(y)$

<center>$$\text{Expectation } E(g(x)) = \sum_{x} g(x)f(x)$$</center>

Short proof sketch:

<center>$$E(g(x))=\sum_{x}y \cdot f^{'}(y) = \sum_{y}\sum_{x \in O_x : g(x)=y}y \cdot f(x) = \sum_{y}\sum_{x:g(x)=y} g(x)f(x) = \sum_{x \in O_x} g(x) f(x)$$</center>

**Expectation of linear functions of random variables**

Let $X$ be a discrete random variable with probability mass function $f$, and let $g$ be a function from the real numbers to the real numbers, $g:\mathbb{R} \rightarrow \mathbb{R}$.

$$\text{With } E(g(X)) = \sum_{x \in O_x} g(x) f(x) \text{, let } g(x) = ax + b$$

<center>$$\text{Expectation } E(g(X)) = \sum_{x \in O_x} (ax+b)f(x) = \sum_x axf(x) + bf(x) = a\sum xf(x) + b\sum f(x) = aE(X)+b = g(E(X))$$</center>

**Variance**
Let $X$ be a random variable and let $\mu=E(X)$

The variance of $X$ is defined as $var(X)=E[(X-\mu)^2]$. Carrying the calculations through,

$var(X)=E[X^2-2X\mu+\mu^2]=E(x^2)-E(2X\mu)+E(\mu^2)=E(X^2)-2\mu E(X)+\mu^2 =E(X^2)-2\mu^2+\mu^2 = E(X^2)-\mu^2=E(X^2)-[E(X)]^2$

Properties

Let $c\in\mathbb{R}$.

1. $var(X+c)=E[(X+c-E(X+c))^2] = E[(X+c-E(X)-c)^2]= var(X)$
2. $var(cX)=E[(cX-E(cX)^2]=E[(cX-cE(X))^2]=E[c^2(X-E(ZX))^2]=c^2E[(x-E(x))^2]=c^2var(X)$

Standard deviation
$\sigma_x=\sqrt{var(x)}$

**Binomial Probability**

A **binomial experiment** is defined by the following characteristics:

1. The experiment consists of a sequence of $n$ smaller experiments called **trials**, where $n$ is fixed in advance of the experiment.
2. Each trial can result in one of the same two possible outcomes (dichotomous trials), which we generically denote by success (S) and failure (F).
3. The trials are independent, so that the outcome on any particular trial does not influence the outcome on any other trial.
4. The probability of success P(S) is constant from trial to trial; we denote this probability by $p$.

Rule.

Consider sampling without replacement from a dichotomous population of size $N$. If the sample size (number of trials) $n$ is at most 5% of the population size, the experiment can be analyzed as though it were exactly a binomial experiment.

Definitions

A **Binomial Random Variable** $X$ associated with a binomial experiment consisting of n trials is defined as

$$X = \text{ the number of }S\text{’s among the }n\text{ trials}$$
Because the pmf of a binomial rv $X$ depends on the two parameters $n$ and $p$, we denote the pmf by $b(x; n, p)$.

$b(x; n, p) = \left\{\begin{array}{{lcl}} \left(^n_x \right)p^x(1-p)^{n-x} &; & x= 0, 1, 2, \dots, n  \\ 0 &; & otherwise \end{array} \right.$

For $X$ ~ $Bin(n, p)$, the cdf will be denoted by
$$B(x; n, p) = P(X \leq x) = \sum_{y=0}^x b( y; n, p) \quad x = 0, 1, \dots , n$$

If $X \text{~ } Bin(n, p)\text{, then }E(X) = np\text{, }V(X)=np(1-p) = npq \text{, and } \sigma_x =  \sqrt{npq}\text{ ( where }q = 1 - p)$.

## Unit 4.2: Continuous Random Variables

Definitions.

A **Continuous Random Variable** $X$ takes on values in a continous interval or set of intervals, but the probability of any one value is zero.

A **Continuous Random Variable** $X$ is described by probability density function $f$ in the problem space $(\mathscr{S}, \mathscr{F}, P)$

<center>$$P(a \leq X \leq b) = \int_{x=a}^b{f(x)dx}$$</center>

The **cumulative probability distribution**, $F(x)$, is the area under f from $-\infty$ to $x$:

<center>$$F(x)=P(X \leq x) = \int_{y=-\infty}^x{f(y)dy}$$</center>

$\text{Noting that } \int_{y=-\infty}^\infty{f(y)dy} = 1$

$\text{By the fundamental theorom of calculus, }  f(x)=F^{'}(x) \text{ when } F^{'}(x) \text{ exists }$


**Expectation of Continuous Random Variables**

<center>$$\text{Expectation of } X =\int_{x=-\infty}^\infty x \cdot f(x)dx$$</center>

$\text{if } h:\mathbb{R}\rightarrow \mathbb{R} \text{ then } h(x) \text{ is a random variable and } E(h(x))=\int_{x=-\infty}^\infty h(x)f(x)dx$

$\text{if }h(x) = ax+b \text{ then } E(h(x)) = aE(x)+b$


**Uniform Random Variable**


X has a uniform distribution on [A,B] if it has probably density function
<center>$$f(x)=\left\{\begin{array}{lcl} \frac{1}{B-A} &; & A \leq x \leq B \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$</center>

```{r uniform_pmf_1, echo=FALSE}
y=c(1, 1)
x=c(2, 3)
plot(x, y, type="l", xlim=c(1,4),ylim=c(0,2), ylab="f(x)", axes= FALSE)
title(quote("Uniform distribution on [A, B]"))
#title(quote("axis(1, .., gap.axis = f)," ~~ f >= 0))
axis(1, 1:4, c(' ', 'A', 'B', ' '))
axis(2)
box()
```

When $X$ is between $A$ and $B, F(x)=\int_{-\infty}^{x}f(y)dy = \int_{A}^{x}\frac{1}{B-A}dy={\frac{y}{B-A}} \bigg\rvert_{a}^{x}=\frac{x-A}{B-A}$

$$F(x) = \left\{\begin{array}{lcl} 0 &; & \mbox{x<A}; \\ \frac{x-A}{B-A}; &; & {A \leq x \leq B}; \\ 1 & ; & B \leq x \end{array} \right.$$


```{r cumulative_pmf_2, echo=FALSE}
y=c(0, 0, 1, 1)
x=c(0, 2, 3, 4)
plot(x, y, type="l", xlim=c(1,4),ylim=c(0,2), ylab="F(x)", axes= FALSE)
title(quote("Cumulative distribution on [A, B]"))
axis(1, 1:4, c(' ', 'A', 'B', ' '))
axis(2)
box()
```

<center>Expectation $E(x) = \int_{-\infty}^{\infty}x \cdot f(x)dx = \int_{A}^{B}x \cdot \frac{1}{B-A}dx = \frac{x^2}{2\cdot(B-A)}\bigg\rvert_{A}^{B}=\frac{B^2-A^2}{2 \cdot(B-A)} = \frac{A+B}{2}$</center>

**Normal Distribution**

A continuous rv X is said to have a normal distribution with parameters $\mu$ and $\sigma$ (or $\mu$ and $\sigma^2$), where $-\infty < \mu < \infty$ and $0 < \sigma$, if the pdf of X is:

$$f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} -\infty < x < \infty$$
It can be shown that $E(X) = \mu$ and $V(X) = \sigma^2$, so the parameters are the mean and the standard deviation of X.

The statement that X is normally distributed with parameters $\mu$ and $\sigma^2$ is often abbreviated $X \sim N(\mu,\sigma^2).$

$\mu$, the location parameter, is both the mean and the median. 

$\sigma$, the scale parameter, stretches the curve horizontally.

Standard Normal, Z, or $\phi(z)$, is when $\mu=0$ and $\sigma=1$

Standardizing a Normal Variable is when we take a normal variable that is not standard normal, and adjust it. The new random variable becomes $\frac{(X-\mu)}{\sigma}$, and we can express probabilities invovling X in terms of the z-distribution.


Properties of the Normal Distribution


68% of the area is within 1 standard deviation of the mean.

95% of the area is within 2 standard deviations of the mean.

99.7% of the area is within 3 standard deviations of the mean.


**Deviations from normality**

***Skewness***

Negative Skew - tail is to the left

Positive Skew - tail is to the right

***Kurtosis***

Platykurtic - flatter and smooshed down

Leptokurtic - much more bunched together and peek up.

\section{1. Distributions}

\subsection{Unit 5: Joint Distributions}

\verb|Joint Probability Density Fxn:| For two continuous rv's $X$ and $Y$.  The joint pdf of $(X,Y)$ is defined $\forall A \subseteq \mathbb{R}^2$
$$  P((X,Y)\in A) = \iint_{A} f(x,y)dx dy $$
It must be that $f(x,y) \geq 0$ and $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) dx dy= 1$. Note also that this integration is commutative.





