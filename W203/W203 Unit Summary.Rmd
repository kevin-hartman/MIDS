---
title: "W203 Unit Summary"
author: "Kevin Hartman"
date: "5/12/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```



## Terms and Definitions
A **population** is a well-defined collection of objects in an area of interest (investigation).

A **census** occurs when desired information is available for all objects in the population.

A **sample** is a subset of the population, selected in some prescribed manner.

A **characteristic** is an attribute of the data, which may be categorical or numerical.

A **variable** is any characteristic whose value may change from one obhject to another in the population.

A **univariate** data set consists of observations on a single variable.

A **multivariate** data set consists of observations on multiple variables.

A **bivariate** data set (a special case of multivariate) is when observations are made on two variables specifically.

**Descriptive Statistics** -- the branch of statistics where an investigator simply wishes to summarize and describe important features of the data.

**Inferential Statistics** -- the branch of statistics where an investigator would like to use the sample information to draw a form of conclusion about the population.


## Unit 1: Descriptive Statistics

**Sample Mean**
<center>$$\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$$</center>
**Sample Median**

After ordering $n$ observations from smallest to largest,
<center>$\tilde{x} = \left\{\begin{array}{lr} \left(\frac{n+1}{2} \right)^{th} \text{ordered value, if n is odd} \\ \text{average of } \left(\frac{n}{2}\right)^{th} \text{and} \left(\frac{n}{2} + 1\right)^{th} \text{ordered values, if n is even} \end{array} \right.$</center>

**Trimmed Mean** - a compromise between $\bar{x}$ and $\tilde{x}$ by eliminating a percentage of the smallest and largest portions of the sample and averaging what remains.

**Range** - the difference betwen the largest and smallest sample variables.

**Deviations from the Mean**

For each value $x_i$ in the sample, its deviation from the mean is $x_i-\bar{x}$

**Sample Variance**

Variance, denoted as $s^2$, is the average squared differences from the mean in a sample set.

<center>$$s^2=\frac{S_{xx}}{n-1}=\frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}$$</center>

It is customary to refer to $s^2$ as being based on ${n-1}$ **degrees of freedom**.

**Sample Standard Deviation**

Standard deviation, denoted as $s$ is the (positive) square root of the variance:

<center>$s = \sqrt{s^2}$</center>


Properties:

Let $x_1, x_2, \dots, x_n$ be a sample and $c$ be any nonzero constant.

1. If $y_1 = x_1 + c, y_2 = x_2 + c,\dots, y_n = x_n + c$, then $s_y^2 = s_x^2$ and
2. If $y_1 = cx_1,\dots,y_n=cx_n$, then $s_y^2=c^2s_x^2$, $s_y=|c|s_x$

where $s_x^2$ is the sample variance of the $x$'s and $s_y^2$ is the sample variance of the $y$'s.


**Upper Fourth** - median of the largest half of observations

**Lower Fourth** - median of the smallest half of observations

**Fourth Spread**

<center>$f_s$ = upper fourth - lower fourth</center>

**Outlier** - any observation farther than $1.5f_s$ from the closest fourth (considered mild if less than $3f_s$)

**Extreme Outlier** - an observation farther than $3f_s$ from the nearest fourth


**Sample Covariance**

The simplest measure of association between two variables.

<center>$cov(x,y)==\frac{\sum_{i=1}^n(x_i-\bar{x})(y_1-\bar{y})}{n-1}$</center>


**Sample Correlation**

Measures the linear relationship betwen to variables

<center>$r_{x,y}=\frac{cov(x,y)}{s_xs_y}$</center>


Properties:

1. Adding $c$ to variable $x$ or variable $y$ doesn't change $r_{x,y}$.
2. Multipling a non-zero $c$ to variable $x$ or variable $y$ doesn't change $r_{x,y}$


## Unit 3: Probability Theory

\usepackage[scr=boondoxo,scrscaled=1.05]{mathalfa}

**Sample space** of an expirement, denoted by $\mathscr{S}$ ,is the set of all possible outcomes of that expirement.

**Event space**, denoted by $\mathscr{F}$, is the subset of outcomes from the sample space. It includes  subsets of $\mathscr{S}$, the empty set $\emptyset$ and $\mathscr{S}$ itself.

**Probability Rule** is a function, $P$, from the set of events to the real numbers

**Axioms**

1. $P(A) >= \emptyset$ for any event $A$ in $\mathscr{F}$
2. $P(\mathscr{S}) = 1$
3. For any countably infite set of disjoint events $\{A_1, A_2, \dots\}$

<center>$P(A_1 \cup A_2 \cup \dots) = \sum_{i=1}^\infty P(A_i)$</center>

**Probability space** is the triple of $(\mathscr{S}, \mathscr{F}, P)$

Properties

1. For any event $A$, $P(A) + P(!A) = 1$, from which $P(A) = 1 - P(!A)$
2. For any event $A$, $P(A) \leq 1$
3. Addition Rule: For any two events $A$ and $B$, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
4. For any three events $A$, $B$, and $C$, $P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$

**Conditional Probability**

1. For any two events $A$ and $B$ with $P(B) > 0$, the conditional probability of $A$ given that $B$ has occured is defined by $P(A|B) = \frac{P(A \cap B)}{P(B)}$
2. Multiplication Rule: $P(A \cap B) = P(B) \cdot P(A|B)$
3. Extending to 3 or more events: $P(A \cap B \cap C) = P(B) \cdot P(A|B) \cdot P(C|A \cap B)$

**Independence**

1. If events $A$ and $B$ are independent from one another, then probability of $A$ given $B$ is the same as the probability of $A$, e.g. $P(A|B) = P(A)$. 
2. Multiplication Rule: $P(A \cap B) = P(A) \cdot P(B)$

**Bayes' Rule**

<center>$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$</center>

**Law of Total Probability**
<center>$P(B) = P(B|A) \cdot P(A) + P(B|!A) \cdot P(!A)$</center>

*Quiz:*

X is a rare disease that affects 2 out of 10,000 people.

Let Y be the event of a positive test.

The best test gives the wrong result 1% of the time for healthy people but is always right for affected people.

If a person tests positive, what is the change that he or she has the disease, P(X|Y)?

$P(X) = 0.0002, P(!X) = 0.9998$

$P(Y|X) = 1$

$P(Y|!X) = 0.01$

$P(Y) = P(Y \cap X) + P(Y \cap !X)$

$P(Y) = P(Y|X) \cdot P(X) + P(Y|!X) \cdot P(!X)$

$P(Y) = (1 \cdot 0.0002) + (0.01 \cdot 0.9998)$

$P(X|Y) = P(Y|X) \cdot P(X) / P(Y)$

$P(X|Y) = \frac{1 \cdot 0.0002}{1 \cdot 0.0002 + 0.01 \cdot 0.9998}$

```{r cakc_this, echo = FALSE}
xy <-{1 * 0.0002} / {1 * 0.0002 + 0.01*0.9998}
xy
```

## Unit 4.1: Discrete Random Variables

Given a Probability space $(\mathscr{S}, \mathscr{F}, P)$, a **random variable** is a function whose domain is the sample space $\mathscr{S}$ and whose range is the set of real numbers.
<center>$X:\mathscr{S} \rightarrow \mathbb{R}$</center>
e.g. $X(\omega) = x$ 

Any random variable whose only possible values are 0 and 1 is called a **Bernoulli random variable**.

A **discrete set** is a set of disconnected points with no continuous intervals. $O = \left \{ {0, 1, 2, \dots } \right \}$. Binary outcomes with values $O = \left \{ {0, 1} \right \}$ where 1 represents success and has probability $p$ is called a **Bernoulli distribution**.

A **Discrete Random Variable** $X$ is described by probability function $f$ in the problem space $(\mathscr{S}, \mathscr{F}, P)$
<center>$f(x) = P(X=x) = P(\left\{w \in \mathscr{S}:X(\omega)=x \right\})$</center>
where the outcomes $O_x = \left \{x \in \mathbb{R} : f(x) > 0 \right \}$

Describing the **Discrete Random Variable** $X$ in terms of a **Cumulative Probability Function**
<center>$$F(X)=P(X \leq x)=\sum_{y \in O_x : y \leq x}{f(y)} \text{   (the sum of probability masses)}$$</center>

Example of a probability function for a **bernoulli random variable**, $O_x = \left \{ {0, 1} \right \}$
<center>$$\text{Let } \alpha = P(X=1), f(x)= \left\{\begin{array}{lcl} 1-\alpha &; & \mbox{x=0}; \\ \alpha; &; & \mbox{x=1}; \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$</center>


**Expectation of Discrete Random Variables**

Let X be a discrete random variable with outcomes $O = \left \{x_1, x_2, \dots, x_k \right \}$ and probability function $f$

<center>$$\text{Expectation } E(X) = \sum_{j=1}^kx_j \cdot f(x_j)$$</center>

**Expectation of random variables with uniform probability distribution**

Let $X$ be a special case where all outcomes have the same probability, e.g. if $f(x_i)=f(x_j) \forall i,j$

<center>$$\text{Expectation } E(x) = \frac{1}{k}{\sum_j x_j}$$</center>

**Expectation of random variables with binary outcomes**

Let $X$ be Bernoulli random variable that has outcomes 0 and 1, e.g. if $$f(x;\alpha) = \left\{\begin{array}{lcl} 1-\alpha &; & \mbox{x=0}; \\ \alpha; &; & \mbox{x=1}; \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$

<center>Expectation $E(X) = \sum_{x \in {0, 1}}x\cdot f(x) = 0 \cdot (1 - \alpha) + 1 \cdot \alpha = \alpha$</center>

**Expectation of geometric random variables**

Let $X$ be geometric random variable for the number of babies born until the first girl, where $G$ is the event the baby is a girl  $$P(G_i) = g \forall i$$

<center>$$f(1)=P(G_1)=g$$</center>
<center>$$f(2)=P(!G_1 \cap G_2)=(1-g) \cdot g$$</center>
<center>$$f(3)=(1-g)^2 \cdot g$$</center>
<center>$$f(x;g)== \left\{\begin{array}{lcl} (1-g)^{x-1} \cdot g &; & x \in \left \{1, 2, \dots \right \}; \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$</center>

<center>$$\text{Expectation } E(X) = \sum_{x \in \left \{ {1, 2 \dots } \right \}}x \cdot (1-g)^{x-1} \cdot g = \frac{1}{g}$$</center>

**Expectation of functions of random variables**

Let $X$ be a discrete random variable with probability mass function $f$ and $g$ be a function from the real numbers to the real numbers, $g:\mathbb{R} \rightarrow \mathbb{R}$

$g(X)$ is a random variable with probability mass function $f^{'}(x)=P(g(X)=x)=\sum_{y \in O_x, g(y)=x}f(y)$

<center>$$\text{Expectation } E(g(x)) = \sum_{x} g(x)f(x)$$</center>

Short proof sketch:

<center>$$E(g(x))=\sum_{x}y \cdot f^{'}(y) = \sum_{y}\sum_{x \in O_x : g(x)=y}y \cdot f(x) = \sum_{y}\sum_{x:g(x)=y} g(x)f(x) = \sum_{x \in O_x} g(x) f(x)$$</center>

**Expectation of linear functions of random variables**

Let $X$ be a discrete random variable with probability mass function $f$, and let $g$ be a function from the real numbers to the real numbers, $g:\mathbb{R} \rightarrow \mathbb{R}$.

$$\text{With } E(g(x)) = \sum_{x \in O_x} g(x) f(x) \text{, let } g(x) = ax + b$$

<center>$$\text{Expectation } E(g(x)) = \sum_{x \in O_x} (ax+b)f(x) = \sum_x axf(x) + bf(x) = a\sum xf(x) + b\sum f(x) = aE(x)+b = g(E(x))$$</center>

**Variance**
Let $X$ be a random variable and let $\mu=E(X)$

The variance of $X$ is defined as $var(X)=E[(X-\mu)^2]$. Carrying the calculations through,

$var(X)=E[X^2-2X\mu+\mu^2]=E(x^2)-E(2X\mu)+E(\mu^2)=E(X^2)-2\mu E(X)+\mu^2 =E(X^2)-2\mu^2+\mu^2 = E(X^2)-\mu^2=E(X^2)-[E(X)]^2$

Properties

Let $c\in\mathbb{R}$.

1. $var(X+c)=E[(X+c-E(X+c))^2] = E[(X+c-E(X)-c)^2]= var(X)$
2. $var(cX)=E[(cX-E(cX)^2]=E[(cX-cE(X))^2]=E[c^2(X-E(ZX))^2]=c^2E[(x-E(x))^2]=c^2var(X)$

Standard deviation
$\sigma_x=\sqrt{var(x)}$

## Unit 4.2: Continuous Random Variables

A **Continuous Random Variable** $X$ takes on values in a continous interval or set of intervals, but the probability of any one value is zero.

A **Continuous Random Variable** $X$ is described by probability density function $f$ in the problem space $(\mathscr{S}, \mathscr{F}, P)$

<center>$$P(a \leq X \leq b) = \int_{x=a}^b{f(x)dx}$$</center>

The **cumulative probability distribution**, $F(x)$, is the area under f from $-\infty$ to $x$:

<center>$$F(x)=\int_{x=-\infty}^x{f(x)dx}$$</center>

$\text{Noting that } \int_{x=-\infty}^\infty{f(x)dx} = 1$

$\text{By the fundamental theorom of calculus, }  f(x)=F^{'}(x) \text{ when } F^{'}(x) \text{ exists }$


**Expectation of Continuous Random Variables**

<center>$$\text{Expectation } (x)=\int_{x=-\infty}^\infty x \cdot f(x)dx$$</center>

$\text{if } h:\mathbb{R}\rightarrow \mathbb{R} \text{ then } h(x) \text{ is a random variable and } E(h(x))=\int_{x=-\infty}^\infty h(x)f(x)dx$

$\text{if }h(x) = ax+b \text{ then } E(h(x)) = aE(x)+b$


**Uniform Random Variable**

(graph of horizontal line from A to B)

X has a uniform distribution on [A,B] if it has probably density function
<center>$$f=\left\{\begin{array}{lcl} \frac{1}{B-A} &; & A \leq x \leq B \\ 0 & ; & \mbox{otherwise} \end{array} \right.$$</center>

for $A \leq x \leq B$

<center>$$F(x)=\int_{-\infty}^{x}f(y)dy = \int_{A}^{x}\frac{1}{B-A}dy={\frac{y}{B-A}} \bigg\rvert_{a}^{x}=\frac{x-A}{B-A}$$</center>


(graph of sloped line from A to B)

$f(x;\alpha) = \left\{\begin{array}{lcl} 0 &; & \mbox{x<A}; \\ \frac{X-A}{B-A}; &; & {A \leq x \leq B}; \\ 1 & ; & B \leq x \end{array} \right.$

<center>Expectation $E(x) = \int_{-\infty}^{\infty}x \cdot f(x)dx = \int_{A}^{B}x \cdot \frac{x}{B-A}dx = \frac{x^2}{2\cdot(B-A)}\bigg\rvert_{A}^{B}=\frac{B^2-A^2}{2 \cdot(B-A)} = \frac{A+B}{2}$</center>
