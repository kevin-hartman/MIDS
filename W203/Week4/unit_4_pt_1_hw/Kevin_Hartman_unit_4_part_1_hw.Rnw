
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Statistics for Data Science (DataSci w203)
% Unit 4 part 1 homework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Setup
\documentclass[12pt,a4paper]{article}
\usepackage[inner=1.5cm,outer=1.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx, subfig}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\numberwithin{equation}{subsection}
\usepackage{hyperref}
\usepackage{listings}


\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Statistics for Data Science \\
       Unit 4 Part 1 Homework: Discrete Random Variables}
\maketitle


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\begin{enumerate}

\item \textbf{Best Game in the Casino}ß†

You flip a fair coin 3 times, and get a different amount of money depending on how many heads you get. For 0 heads, you get \$0. For 1 head, you get \$2. For 2 heads, you get \$4. Your expected winnings from the game are \$6. 

\textbf{GIVENS}

X is a binomial random variable based on $n$ trials with success probability $p$.

Let x be the number of H's among the $n=3$ trials.

$$b(x;n,p) = \begin{cases}
(^n_k)p^x(1-p)^{n-x}, &x \in \{0,1,2,3,...\}\\
0, &otherwise.\\
\end{cases}
$$

$$\text{Probablity of 0 Heads: }P(X=0)=b(0;3,\frac{1}{2}) = (^3_0)(\frac{1}{2})^3 = \frac{1}{8}$$
$$\text{Probablity of 1 Heads: }P(X=1)=b(1;3,\frac{1}{2}) = (^3_1)(\frac{1}{2})^3 = \frac{3}{8}$$
$$\text{Probablity of 2 Heads: }P(X=2)=b(2;3,\frac{1}{2}) = (^3_2)(\frac{1}{2})^3 = \frac{3}{8}$$
$$\text{Probablity of 3 Heads: }P(X=3)=b(3;3,\frac{1}{2}) = (^3_3)(\frac{1}{2})^3 = \frac{1}{8}$$

<<echo=FALSE, results=hide>>=
x = seq(0,3,by=1)
y = dbinom(x, size = 3,prob=.5)
@

<<label=xbinom, fig=TRUE, echo=FALSE, pdf=FALSE, eps=TRUE, png=TRUE, include=FALSE>>=
plot(x,y, type="h", col=2, ylab="P(x)", xlab="x", ylim=c(0,0.4125))
abline(v=1.5, col="blue", lty=3)
@
\begin{minipage}{\linewidth}
  \centering
  \includegraphics[width=6cm]{Kevin_Hartman_unit_4_part_1_hw-xbinom}
  \captionof{figure}{Plot of binomial distribution of n=3, p=$\frac{1}{2}$}
\end{minipage}

\begin{enumerate}
\item How much do you get paid if the coin comes up heads 3 times?

\subsection*{
$\text{Given fair coins, the expectation of X, } E(X) = np = 3 \cdot \frac{1}{2} = \frac{3}{2} \text{ (also shown in graph above)}$

The expected winnings from the game is \$6, such that $G(E(X)) = 6$

And we have:

$G(X=0) = 0$

$G(X=1) = 2$

$G(X=2) = 4$

$G(X=3) = A$

$$G(E(X)) = \sum_{y \in O_x; y \leq x} p(y) \cdot g(y) = 6$$

$(\frac{1}{8} \cdot 0) + (\frac{3}{8} \cdot 2) + (\frac{3}{8} \cdot 4) +  (\frac{1}{8} \cdot A) = 6$

$(\frac{6}{8}) + (\frac{12}{8}) + (\frac{A}{8} = 6$

$6 + 12 + A = 48$

$A = 30$

}
\item Write down a complete expression for the cumulative probability function for your winnings from the game.

0  2   2  36

0  1   2   3



\end{enumerate}

\item \textbf{Reciprocal Dice}

Let $X$ be a random variable representing the outcome of rolling a 6-sided die.  Before the die is rolled, you are given two options:

\begin{enumerate}
\item You get $1/E(X)$ in dollars right away.
\item You wait until the die is rolled, then get $1/X$ in dollars.
\item Which option is better for you, in expectation?
\end{enumerate}



\item \textbf{The Baseline for Measuring Deviations}

Given any random variable $X$ and a real number $t$, we can define another random variable $Y = (X - t)^2$. In other words, for any random variable $X$, we can choose a real number, $t$, as a baseline and calculate the squared deviation of $X$ away from $t$.

You might wonder why we often square deviations (instead of taking an absolute value, or cubing them, etc.).  This exercise will shed some light on why this is a natural choice.

\begin{enumerate}
\item Write down an expression for $E(Y)$ and simplify it as much as you can.  Even though we haven't proved this yet, you can use the fact that for any two random variables, $A$ and $B$, $E(A + B) = E(A) + E(B)$.
\item Taking a partial derivative with respect to $t$, compute the value of $t$ that minimizes $E(Y)$.  (Hint: Your answer should be a very familiar value)
\item What is the value of $E(Y)$ for this choice of $t$?
(Hint: this should also be a very familiar value)
\end{enumerate}

\item \textbf{Optional Advanced Exercise: Heavy Tails}

One reason to study the mathematical foundation of statistics is to recognize situations where common intuition can break down.  An unusual class of distributions are those we call \textit{heavy-tailed}.  The exact definition varies, but we'll say that a heavy-tailed distribution is one for which not all moments are finite.  Consider a random variable $M$ with the following pmf:

$$p_M(x) = \begin{cases}
c/x^3, &x \in \{1,2,3,...\}\\
0, &otherwise.\\
\end{cases}
$$

where $c$ is a constant (you can calculate its value if you like, but it's not important).

\begin{enumerate}
\item Is $E(M)$ finite?

Yes it is finite, but not particularly useful. The mean doesn't represent either side of the population very well.

\item Is $V(M)$ finite?

Also finite, and also not useful. I would think you may need to take a log of the function above or use some other mechanism to make the result more linear when computing the mean. Then variance would be more meaningful after taking the square root.

\end{enumerate}

Heavy-tailed distributions may seem odd, but they're not as rare as you might suspect.  Researchers argue that the distribution of wealth is heavy-tailed; so is the distribution of computer file sizes, insurance payouts, and area burned by forest fires.  These random variables are problematic in that a lot of common statistical techniques don't work on them.  For this class, we'll assume that all of our variables don't have heavy-tails.

\end{enumerate}

\end{document}