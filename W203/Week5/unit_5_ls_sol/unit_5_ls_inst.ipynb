{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Unit 5 Live Session </h1>\n",
    "<h2> W203 Instructional Team </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Distribution of Random Variables </h2>\n",
    "<a ><img src=\"https://i.imgflip.com/2tiaeo.jpg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 0.1 Class Announcements </h3>\n",
    "1. Announcement 1\n",
    "2. Announcement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 0.2 Getting to This Document</h3>\n",
    "\n",
    "If you have not cloned the unit_5_ls_sol repo yet then on the command line\n",
    "\n",
    "1. git clone https://github.com/w203-spring-19/unit_5_ls_sol.git \n",
    "\n",
    "2. cd unit_5_ls_sol\n",
    "\n",
    "\n",
    "\n",
    "If you have cloned this repo already then on the command line\n",
    "\n",
    "1. cd unit_5_ls_sol\n",
    "\n",
    "2. git fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1 Discrete Random Variables Review </h3>\n",
    "\n",
    "Last time we discussed random variables in a general way by emphasizing that functions of  random variable are themselves random variables,\n",
    "\n",
    "** 1.1 Marginal Distributions: ** \n",
    "\n",
    "Suppose that a random variable $X$ takes on $N_X$ values, and $Z = f(X)$ for some function $f(\\cdot)$ which results in $Z$ taking on $N_Z\\leq N_X$ values. Our previous discussion allows us to write the following\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(Z) = \\sum_{i=1}^{N_Z}Z_i \\cdot P(Z = Z_i) \\hspace{1cm} \\text{ and also } \\hspace{1cm} E(Z) = E(f(X)) = \\sum_{i=1}^{N_x}f(X_i) \\cdot P(X = X_i) \n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.2 Binomial Distribution: **\n",
    "\n",
    "Suppose we are repeating an experiment having only two outcomes $ \\{\\text{success},\\text{failure}\\}$, $N$ times in such are way that their outcomes are independent where $p = P(\\text{success})$, then \n",
    "\n",
    "$$ P(\\text{ k successes in N trials }) = {{N}\\choose{k}}p^k(1-p)^{N-k} \\;\\;\\; \\text{ where } \\;\\;\\; {{N}\\choose{k}} = \\frac{N!}{(N-k)!k!}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.3 Joint Distributions: **\n",
    "\n",
    "Now, suppose we have two random variables $X$ and $Y$ which take on $N_X$ and $N_Y$ values respectively, We calculate some of the characteristics of their joint distributions as follows. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(Y|X = X_i) &= \\sum_{j = 1}^{N_Y} Y_j \\cdot P(Y = Y_j | X = X_i)\\\\[3pt]\n",
    "E(XY) &= \\sum_{i=1}^{N_X} \\sum_{j=1}^{N_Y} X_iY_j\\cdot P(X_i,Y_j) \\\\[5pt]\n",
    "\\text{cov}(X,Y) & = E(XY) - E(X)E(Y)\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.4 Law of Iterated Expectations: **\n",
    "\n",
    "One thing that will make your life immensely easier is the law of iterated expectations \n",
    "\n",
    "$$ E(Y) = E[E(Y|X)] $$\n",
    "\n",
    "The conditional expectation of any r.v. $Y$ conditional on another discrete and finite r.v. $X$ is a function of that r.v. this means that there exists some function $f(\\cdot)$ such that \n",
    "\n",
    "$$E(Y|X) = f(X)$$ \n",
    "\n",
    "as a result we can use the equation from above to write. \n",
    "\n",
    "$$ E(Y) = E[E(Y|X)] = \\sum_{i=1}^{N_x} E(Y|X = X_i)P(X=X_i) $$  \n",
    "\n",
    "So if we know $P(X=X_i)$, and $E(Y|X=X_i)$ is easier to figure out than $P(Y=Y_j)$ then the Law of iterated expectations will save us some effort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** How can we characterize the random variable $X$? First define the probability mass function. Then compute the expectation. What is a real-world setting in which you might encounter a discrete RV of this sort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a discrete uniform random variable, just like the fair die we encountered earlier. \n",
    "\n",
    "> More complex distributions could be used to characterize situations in which, for example, receiving a greater number of questions is less likely than receiving fewer questions.\n",
    "\n",
    "$$ \n",
    "P(X = a) = \n",
    "\\begin{cases} \n",
    "    1/3 & \\text{ if } a = 0 \\\\\n",
    "    1/3 & \\text{ if } a = 1 \\\\\n",
    "    1/3 & \\text{ if } a = 2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "> Next,What quick computation can you do to demonstrate this is a pmf?\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "E(X) = \\sum_{i=1}^{N_x} X_iP(X = X_i) = 0\\cdot1/3 + 1\\cdot1/3 + 2\\cdot1/3 = 1\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "\n",
    "> For a real world setting, number of requests made to a customer service representative during a call, among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** How should we characterize the random variable $Y$? Note that the pmf of Y has been defined for us in terms of the outcomes of $X$. Describe a real-world setting in which you could encounter jointly distributed RVs of this sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can think of this discrete RV as characterizing success or failure in responding to a question, conditional on the number of questions. The underlying model here is that of the Bernoulli random variable. See examples 3.1 and 3.2 in the Devore for an introduction. We can imagine this variable capturing the number of call center requests for which the customer is dissatisfied with the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Compute the expectation of $Y$, conditional on $X$, $E(Y|X)$ First remind yourself of the definition of conditional expectation. What does this expression tell us?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(Y=0|X=0) &= 1 \\\\\n",
    "P(Y=1|X=0) &= 0 \\\\\n",
    "P(Y=2|X=0) &= 0 \\\\[15pt]\n",
    "P(Y=0|X=1) &= 1-1/4 = 3/4 \\\\\n",
    "P(Y=1|X=1) &= 1/4 \\\\\n",
    "P(Y=2|X=1) &= 0 \\\\[15pt]\n",
    "P(Y=0|X=2) &= {{2}\\choose{0}}(1/4)^0(1-1/4)^{2-0} = (3/4)^2 = 9/16 \\\\\n",
    "P(Y=1|X=2) &= {{2}\\choose{1}}(1/4)^1(1-1/4)^{2-1}= 2(1/4)(3/4)^2 = 6/16 \\\\\n",
    "P(Y=2|X=2) &= {{2}\\choose{2}}(1/4)^2(1-1/4)^{2-2} = 1/16 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> The conditional expectation gives an expression for the expected value of $Y$ given all of the possible values of $X$. Another way to describe this is that the conditional expectation tells us how our random variable $Y$ will behave within the levels defined by the random variable $X$. If $X$ were continuous, characterizing this conditional expectation would be cumbersome. but for $X$ discrete, we can do so by setting $X$ to each of its possible values, and computing the expectation of $Y$ given that value of $X$. In symbols:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(Y|X=0) &= 0\\cdot P(Y=0|X=0) + 1\\cdot P(Y=1|X=0)+ 2\\cdot P(Y=2|X=0) \\\\\n",
    " &= 0\\cdot 0 + 1\\cdot 0 + 2\\cdot 0 = 0\\\\[15pt]\n",
    "E(Y|X=1) &= 0\\cdot P(Y=0|X=1) + 1\\cdot P(Y=1|X=1)+ 2\\cdot P(Y=2|X=1) \\\\\n",
    " &= 0\\cdot 3/4 + 1\\cdot 1/4 + 2\\cdot 0 = 1/4 \\\\[15pt]\n",
    "E(Y|X=2) &= 0\\cdot P(Y=0|X=2) + 1\\cdot P(Y=1|X=2)+ 2\\cdot P(Y=2|X=2) \\\\\n",
    " &= 0\\cdot 9/16 + 1\\cdot 6/16 + 2\\cdot 1/16 = 1/2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Where do we find the probablities of each of these terms? We can build a tree and use the axioms of probability!\n",
    "\n",
    "> Also, note a simple and intuitive way of describing this same expectation. Let Z be Bernoulli that defines answering question $i$ incorrectly. \n",
    "\n",
    "$$\n",
    "E(Y|X) = E\\left(\\sum_{i}{Z_{i}}\\right) = \\sum_{i}E\\left(Z_{i}\\right) = \\sum_{i}\\frac{1}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Using the law of iterated expectations, compute $E(Y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Conceptually, the law of iterated expectations allows us to learn about the central tendency of a random variable, even when we only observe that variable jointly with another random variable. The expression for the law of iterated expectations is\n",
    "\n",
    "$$\n",
    "E(E(Y|X)) = E(Y)\n",
    "$$\n",
    "\n",
    "> Look at the inner expectation. This is the same conditional expectation we were talking about a moment ago. Here we can compute this conditional expectation easily by hand, because we are conditioning on a discrete RV that takes only a few values. The result of that inner expectation will be a series of terms that depend on X. With those in hand, we can compute the outer expectation. I'll give the general expression, and leave it to you to compute the probabilities.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(Y) &= E(Y|X=0)P(X=0) + E(Y|X=1)P(X=1) + E(Y|X=2)P(X=2) \\\\\n",
    "& = 0 \\cdot1/3 + 1/4 \\cdot 1/3 + 1/2\\cdot 1/3 \\\\\n",
    "& = 1/4\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Look at the above expression. Can you see how it is simply an application of the definition of expectation? Do you see also that this results aligns with the one we arrived at previously?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Describe the joint probability distribution of $X$ and $Y$.  (You may find it easiest to use a table). For an introduction to joint pdfs of discrete RVs, see example 5.1 in Devore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|P(x,y)| x=0    |   x=1  |  x=2  | p(y)  |\n",
    "|------|--------|--------|-------|-------|\n",
    "| y=0  | 16/48  | 12/48  | 9/48  | 37/48 |\n",
    "| y=1  |  0     | 4/48   | 6/48  | 10/48 |\n",
    "| y=2  |   0    |  0     | 1/48  | 1/48  |\n",
    "| p(x) | 1/3    |   1/3  |  1/3  |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Compute the expectation of the product of $X$ and $Y$, $E(XY)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Could compute using law of iterated expectations, together with expression fo $E(Y|X)$ that we derive above. Or could compute directly using expression for the expectation of a product of random variables. See example 5.13 in the Devore. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(XY) &= \\sum_{i=1}^{N_X} \\sum_{j=1}^{N_Y} X_iY_j\\cdot P(X_i,Y_j) \\\\\n",
    "& = 1\\cdot1\\cdot 12 + 1\\cdot 2 \\cdot 2/16 + 2\\cdot 2\\cdot 1/16\\cdot 1/3 \\\\\n",
    "& = 5/12\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "> Where would we find the entries from this table? Can you plug them in?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Using the previous result, compute $\\text{cov}(X,Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we make use of an important result about covariance. See examples 5.15 and 5.16 in the Devore textbook to review.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{cov}(X,Y) &= E(XY) - E(X)E(Y) \\\\\n",
    "&= 5/12 - [0\\cdot 1/3 + 1\\cdot 1/3 + 2 \\cdot 1/3]\\cdot 1/4 \\\\\n",
    "&= 5/12 - 1/4 = 1/6\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3 Joint Distribution Practice:  Triangular Regions </h3>\n",
    "\n",
    "Continuous random variables $X$ and $Y$ have a joint distribution with probability density function,\n",
    "\n",
    "$$ f(X,Y) = \\begin{cases}\n",
    "1 & \\text{ if } \\;0 < Y < 1 \\;\\text{ and } \\; a \\cdot Y < X < a \\cdot Y + 1 \\\\\n",
    "0 & \\text{ if otherwise.}\n",
    "\\end{cases} $$\n",
    "\n",
    "where $a$ is a constant.\n",
    "\n",
    "**Note:** For a problem like this, we have to characterize the RVs jointly before we can talk about them separately. A real-world situation that we might want to model using jointly distributed continuous random variables like these could be the results of two service requests, where the initial service request $Y$ is typically shorter, and the follow up request, $X$, is typically longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Choose 2 example values for $a$ and draw a graph of the region for which $X$ and $Y$ have positive probability density. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1tvGyF6MIFJyLrGdDY3D9s6Z4EPsBfjFC\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Derive the marginal distribution of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f(Y) = \\int_{-\\infty}^{\\infty} f(X,Y)dX = \\int_{aY}^{aY+1} 1 \\cdot dX = [\\;X\\;]_{aY}^{aY+1} = aY+1 - aY = 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Compute the conditional expectation of $X$, conditional on $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "E(X|Y) &= \\int_{-\\infty}^{\\infty} X\\;f(X|Y) dX \\\\ \n",
    "       &= \\int_{-\\infty}^{\\infty}X \\; \\frac{f(X,Y)}{f(Y)}dX\\\\\n",
    "       &= \\int_{aY}^{aY+1} X \\cdot dX \\\\\n",
    "       &= [\\;1/2 \\cdot X^2\\;]_{aY}^{aY+1} \\\\[5pt]\n",
    "       &= 1/2( (aY+1)^2 - (aY)^2) = \\\\[5pt]\n",
    "       &= 1/2\\cdot[a^2Y^2 + 2aY + 1 - a^2Y^2] \\\\[5pt]\n",
    "       &= aY+1/2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** As a slight variation on the previous part, compute $E(XY | Y)$.  Note that since we're conditioning on Y, the Y inside the expectation is just a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E(YX|Y) = YE(X|Y) = aY^2+1/2 \\cdot Y$$\n",
    "\n",
    "$$ E(YX|Y=b) = E(bX|Y=b) = bE(X|Y=b)= bf(b) = Yf(Y) $$\n",
    "\n",
    "$$ E(XY) = E(E(XY|X))= E(E(XY|Y)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** Derive $\\text{cov}(X,Y)$.  Hint: an nice way to do this is to use the law of iterated expectations.  Write down the definition of covariance, then break the expectation up into two expectations.  The inner expectation should be conditional on Y, and the outer expectation should be unconditional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> first we calculate\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(Y^2) &= \\int_{0}^1Y^2dY = [1/3\\cdot Y^3 ]_0^1 = 1/3 \\\\\n",
    "E(Y) &= \\int_{0}^1YdY = [1/2\\cdot Y^2 ]_0^1 = 1/2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\text{cov}(X,Y) &= E(XY) - E(X)E(Y) \\\\[5pt]\n",
    "&= E\\left[E(XY|Y) \\right] - E[E(X|Y)]E(Y) \\\\[5pt]\n",
    "&= E\\left[aY^2+1/2 \\cdot Y \\right] - E[aY+1/2]E(Y)\\\\[5pt]\n",
    "& =aE \\left[Y^2\\right] + 1/2E\\left[Y\\right] -(aE[Y] + 1/2)E[Y]   \\\\[5pt]\n",
    "&= a\\cdot 1/3 + 1/2\\cdot 1/2 - (a/2+1/2)\\cdot(1/2) \\\\[5pt]\n",
    "&= a/12\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6** Check what $cov(X,Y)$ equals when $a = 0$.  What is $cov(X,Y)$ when $a = -1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we use the result of 2.5 and calculate\n",
    "\n",
    "$$ \\text{cov}(X,Y;a=0) = 0 \\hspace{2cm} \\text{cov}(X,Y;a=-1) = -1/12 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7** Choose an example value for a, then use R to simulate 100 draws from the given joint distribution and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- 1/2\n",
    "n <- 1000\n",
    "xvals <- as.numeric(NULL)\n",
    "yvals <- as.numeric(NULL)\n",
    "for(i in 1:n){\n",
    "    ti <- runif(1)\n",
    "    yvals[i] <- ti\n",
    "    xvals[i] <- runif(1, min = a*ti, max = (a*ti)+1) \n",
    "}\n",
    "plot(xvals, yvals, xlim = c(0,2), ylim = c(0,2), pch = 19, col = grey(0.5,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4 Discussion on Models </h3>\n",
    "\n",
    "<center>![title](dog_model.png)</center>\n",
    "\n",
    "** Reading: ** \n",
    "\n",
    "The End of Theory: The Data Deluge Makes the Scientific Method Obsolete\n",
    "\\newline\n",
    "<http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory>\n",
    "\n",
    "*All models are wrong, but some are useful.* - George Box, Statistician\n",
    "\n",
    "*All models are wrong, and you can increasingly succeed without them.* - Peter Norvig, Google's Research Director\n",
    "\n",
    "Setting aside the over-dramatic tone of Anderson's piece, there are many elements of his argument worth discussing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Are the fancy algorithms Anderson points to actually model-free?  For example, is the page rank algorithm model-free?  What is a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Page rank is an interesting choice of algorithm because it has an elegant mathematical interpretation (the\n",
    "wikipedia page has a quick primer for those interested). For one thing, the page rank solution can be expressed\n",
    "algebraically as an eigenvector of a modified adjacency matrix (though computing the solution algebraically\n",
    "would be too computationally expensive for most applications). The page rank can also be understood as the\n",
    "result of a particular stochastic process: A web surfer begins at a page at random, then chooses a random\n",
    "link to arrive at a new page. With some probability, the surfer gets bored and stops following links. The\n",
    "page rank equals the probability that the surfer lands on a given page. All in all, far from being model-free,\n",
    "page rank is quite firmly rooted in the type of theory Anderson seems to be criticizing.\n",
    "\n",
    "> Students may have different definitions of what a model is, but here’s one important point: A model is\n",
    "something we use to stand in for the real world to describe, explain, or predict data, since we don’t know\n",
    "everything about the real world.\n",
    "\n",
    "> (real world –>) data <— model\n",
    "\n",
    "> The point is that just because a fancy learning algorithm may not look like our elegant statistical models, it still (1) represents the real world that generates data (2) simplifies the real world in some way (3) has its own behavior (implies a joint probability distribution over all possible data points), even if we can’t characterize that behavior in nice closed-form solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** What is the real difference between parametric statistics and learning algorithms that originate in a computer science tradition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A machine learning algorithm may not look like a statistical model, because it’s often described in computational\n",
    "steps, instead of in simple equations. Still, the algorithm can be fit to data, and then can be used to\n",
    "generate predictions, etc.\n",
    "\n",
    "> Easy answer: Statistical models are generally easy for humans to understand and reason about. There\n",
    "are a small number of parameters, and we can understand how those relate to behavior. By contrast, ML\n",
    "algorithms are often very hard to understand. A random forrest has thousands of values that are fit in the\n",
    "learning process. In fact, researchers have to use separate diagnostic tools, just to study which variables in a\n",
    "data set are actually important for classification.\n",
    "\n",
    "> More subtle answer: A statistical model comes with a lot of structure that lets us do things like compare different models to each other. It’s hard to compare two random forrests or two neural nets together. But in this class we will do things like add a new variable to a linear model, observe how far the coefficient is from zero, and even argue about whether that difference could just be noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** In part, Anderson argues that we don't need models that are understandable by humans.  When is it enough to have a model that \"fits\" the data well, and when do we really need it to be human-readable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At times, we really may just be interested in prediction, and in this case, human readability might not matter.\n",
    "Think about using an algorithm to read handwritten numbers on checks. We don’t care what makes a 3 a 3,\n",
    "we just need to know what number it is.\n",
    "\n",
    "> One important concern arises when we want to actually change something in the real world. We may have a model that predicts when a customer will leave our company for example. It could be really good at this prediction task, but that doesn’t mean that we know how to keep the customer from leaving. That depends on what variables we’ve measured. Maybe a long phone call with tech support is a good predictor of a customer leaving, but we can’t just hang up to limit the length of the phone call and hope they stay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** When does context matter in data science? Is domain knowledge important or can we just rely on the numbers in a data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To go back to the page rank example, google choose one algorithm out of an infinite set of possibilities. How\n",
    "did they know this one would be worth trying? Was it because the model matched intuition for how websites\n",
    "and surfers behave?\n",
    "\n",
    "> Our algorithms are always vulnerable to measurement error, selection bias, and other issues that will not be apparent in the numbers alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5** Anderson seems to imply that we don’t need to test hypotheses today.  Are hypotheses still relevant to data science, or should our focus be on estimating effect sizes, or using our models to classify and predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we’ll argue later in the course, researchers often do over-rely on hypothesis tests or misapply them, and it\n",
    "is important to actually measure the effect size. What is the improvement caused by our new cancer drug?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
