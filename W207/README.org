#+TITLE: Applied Machine Learning 
#+OPTIONS: toc:nil 

* Schedule

| Week | Topics                                    | Async Reading                                   | Optional Reading                                                            | Assignment Due           |
|------+-------------------------------------------+-------------------------------------------------+-------------------------------------------------------------------------+--------------------------|
|    1 | Introduction                              | [[http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf][ Halevy, Norvig, Pereira, The unreasonable effectiveness of data]] |    | -- |
|    2 | Problem Setup and Nearest Neighbors       | [[./Week02/Feynman.1974.pdf][Feynman, R. (1974, June). Cargo cult science. Engineering and Science 37(7).]], [[./Week02/cacm12.pdf][Domingos, A few useful things to know about machine learning]] | [[./Week02/ci0342472.pdf][ Hawkins, The problem of overfitting]] | -- |
|    3 | Supervised Learning I: Naive Bayes        | [[http://www.paulgraham.com/spam.html][Paul Graham on Naive Bayes (2002)]] | [[./Week03/em.pdf][Michael Collins tutorial on Naive Bayes (with math), see pages 1–4]], [[http://norvig.com/spell-correct.html][Norvig, How to write a spelling corrector]] | -- |
|    4 | Supervised Learning II: Decision Tree     | [[http://blog.yhat.com/posts/predicting-customer-churn-with-sklearn.html][blog post from yhat about predicting churn]] | [[./Week04/info-lec.pdf][Carter, An introduction to information theory and entropy]], [[./Week04/IntroToBoosting.pdf][Freund and Schapire, A Short introduction to Adaboost]], [[./Week04/delgado14a.pdf][Delgado, et al, Do we need hundreds of classifiers to solve real-world problems?]] | -- |
|    5 | Supervised Learning III: Regression       |  | [[./Week05/euclid.ss.1009213726.pdf][Breiman, Statistical Modeling: The Two Cultures]], [[./Week05/OLSDerivation.pdf][Deriving Least Squares]], [[./Week05/Freedman_1991A.pdf][Freedman, Statistical Models and Shoe Leather]] | -- |
|    6 | Supervised Learning IV: Linear Models     | [[./Week06/ciml-v0_9-ch06.pdf][Hal Daume, Gradient Descent (chapter 6)]] | [[./Week06/tricks-2012.pdf][Bottou, Stochastic Gradient Descent Tricks]] | -- |
|    7 | Supervised Learning V: Neural Networks    | [[./Week07/ciml-v0_9-ch08.pdf][Hal Daume, Neural Networks (chapter 8)]] |  [[./Week07/NatureDeepReview.pdf][LeCun, et al, Deep Learning]] | -- |
|    8 | Supervised Learning VI: SVMs, Choosing Classifiers, Speech Recognition     | [[./Week08/caruana.icml06.pdf][An Empirical Comparison of Supervised Learning Algorithms]] | [[./Week08/lecture-27.pdf][Cosma Shalizi SVM lecture notes]], [[./Week08/On%20Comparing%20Classifiers%20Pitfalls%20to%20Avoid%20and%20a%20recommended%20approach.pdf][On comparing classiﬁers: Pitfalls to avoid and a recommended approach]], [[https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html][SKLearn classifier comparisons for toy problems]] | -- |
|    9 | Unsupervised Learning I: Cluster Analysis |   | [[./Week09/220451867_Hierarchical_Clustering_Algorithms_for_Document_Datasets.pdf][Zhao and Karypis, Hierarchical clustering algorithms for document datasets]], [[./Week09/MIT_genomics_eisen_reading.pdf][Eisen et al., Cluster analysis and display of genome-wide expression patterns]]  | -- |
|   10 | Unsupervised Learning II: Expectation Maximization    | [[./Week10/em.pdf][Tibshirani lecture notes on EM]] | [[./Week10/ReynoldsRose.pdf][Doug Reynolds original paper on GMMs for speaker identification]] | -- |
|   11 | Unsupervised Learning III: Dimensionality Reduction |  | [[./Week11/ciml-v0_9-ch13.pdf][Hal Daume, Unsupervised Learning (chapter 13)]], [[./Week11/Eigenface.pdf][Turk and Pentland, Eigenfaces for Recognition]] | -- |
|   12 | Network Analysis                          | [[http://infolab.stanford.edu/~backrub/google.html][Page and Brin, The Anatomy of a Large-Scale Hypertextual Web Search Engine]]| [[./Week12/623.pdf][Barabasi, The Scale-Free Property]]  | -- |
|   13 | Recommender Systems                       | |  [[./Week13/a13-gomez-uribe.pdf][Gomez and Hunt, The Netflix Recommender System: Algorithms, Business Value, and Innovation]], [[./Week13/p447-koren.pdf][Koren, Collaborative Filtering with Temporal Dynamics]] | -- |
|   14 | Wrap-Up                                   |                                                 |                                                                          | -- |
|      |                                           |                                                 |                                                                          |    |

* Description 
Machine learning is a rapidly growing field at the intersection of computer science and statistics and concerned with finding patterns in data. It is responsible for tremendous advances in technology, from personalized product recommendations to speech recognition in cell phones. The goal of this course is to provide a broad introduction to the key ideas in machine learning. The emphasis will be on intuition and practical examples rather than theoretical results, though some experience with probability, statistics, and linear algebra will be important. Through a variety of lecture examples and programming projects, students will learn how to apply powerful machine-learning techniques to new problems, how to run evaluations and interpret results, and how to think about scaling up from thousands of data points to billions.

* Prerequisites
  1. Students must have completed the following core data science courses prior to enrollment:
    1. Research Design
    2. Storing and Retrieving Data
    3. Exploring and Analyzing Data
  2. Undergraduate-level probability and statistics. Linear algebra is recommended.
  3. Programming experience in Python. Homework will often require students to consult the [[http://scikit-learn.org/stable/index.html][scikit-learn]] library documentation.

* Assignments and Grading
Course grades will be based mostly on three guided programming projects designed to synthesize concepts introduced in the lectures and one more open-ended final project. Please see [[./Assignments][this page]] for more details.


Course Resources
Most textbooks on machine learning are written with considerable technical detail. As a result, there is no one textbook that aligns with this course. We will list readings that correspond to each week, including some general philosophy and landmark research papers, as well as few chapters from [[http://ciml.info/][Hal Daume’s unfinished textbook]].


* Office Hours 

| *Day*     | *Time*      | *Instructor* | 
|-----------+-------------+--------------|
| Monday    |   | [[https://zoom.us/j/757560269][Alex]]         |
| Tuesday   |   |        |
| Wednesday |   |        |
| Wednesday |   | [[https://zoom.us/j/385112665][Alex]]         |
| Thursday  |   |         |
 
* Grading 
- 3 Projects: 60%
- Final project: 35%
- Participation: 5%

This page was created from the course syllabus located here: https://docs.google.com/document/d/1nwsfta3loT3paoXA4uGUK9RXSAi4mGz7QE24DuXlrgs/edit?usp=sharing
